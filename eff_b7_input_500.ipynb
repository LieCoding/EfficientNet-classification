{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入尺寸改为500，v100训练大概三小时\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本baseline采用pytorch框架，应用ModelArts的Notebook进行开发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集获取\n",
    "将您OBS桶中的数据文件加载到此notebook中，将如下代码中\"obs-aifood-baseline\"修改成您OBS桶名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.15.1-92d9ed92\n",
      "INFO:root:Using OBS-Python-SDK-3.1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "# xxx最好换成自己的obs路径\n",
    "#预训练模型\n",
    "mox.file.copy_parallel('s3://xxxx/premodel/efficientnet-b7-dcc49843.pth','pre.pth')\n",
    "# 大赛数据，改成自己的\n",
    "mox.file.copy_parallel('s3://xxx/data/aifood','aifood')\n",
    "# ranger优化器安装文件\n",
    "mox.file.copy_parallel('s3://xxx/data/ranger.zip','ranger.zip')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压ranger文件\n",
    "!unzip ranger.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aifood\taifood.zip  pre.pth  Ranger-Deep-Learning-Optimizer  ranger.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/54/2e/df11ea7e23e7e761d484ed3740285a34e38548cf2bad2bed3dd5768ec8b9/pip-20.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 73.9MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 9.0.1\n",
      "    Uninstalling pip-9.0.1:\n",
      "      Successfully uninstalled pip-9.0.1\n",
      "Successfully installed pip-20.1\n",
      "/home/ma-user/work/Ranger-Deep-Learning-Optimizer\n",
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Obtaining file:///home/ma-user/work/Ranger-Deep-Learning-Optimizer\n",
      "Requirement already satisfied: torch in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from ranger==0.1.dev0) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "#安装ranger优化器，\n",
    "# refer https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "# 有时候git不了，我直接下载了\n",
    "# !git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "!pip install --upgrade pip\n",
    "%cd Ranger-Deep-Learning-Optimizer\n",
    "!pip install -e . \n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ttach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\r\n",
      "Requirement already satisfied: efficientnet-pytorch in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (0.6.3)\r\n",
      "Requirement already satisfied: torch in /home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages (from efficientnet-pytorch) (1.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "# 安装efficientnet\n",
    "# refer https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/Ranger-Deep-Learning-Optimizer\n",
      "/home/ma-user/work\n"
     ]
    }
   ],
   "source": [
    "%cd Ranger-Deep-Learning-Optimizer\n",
    "from ranger import Ranger  # this is from ranger.py\n",
    "from ranger import RangerVA  # this is from ranger913A.py\n",
    "from ranger import RangerQH  # this is from rangerqh.py\n",
    "%cd ..\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "# import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft = EfficientNet.from_name('efficientnet-b5') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集，并将其分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'三明治': 0, '冰激凌': 1, '土豆泥': 2, '小米粥': 3, '松鼠鱼': 4, '烤冷面': 5, '玉米饼': 6, '甜甜圈': 7, '芒果班戟': 8, '鸡蛋布丁': 9}\n",
      "train_length:4500,val length:500\n"
     ]
    }
   ],
   "source": [
    "# 手动写一个类来读取数据\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "size = 500\n",
    "# 使用image net的mean std 简单的数据增强\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transformer_ImageNet = transforms.Compose([\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    " \n",
    "val_transformer_ImageNet = transforms.Compose([\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "# 目录文件\n",
    "data_dir = 'aifood/images'\n",
    "# 为了划分数据集，和自定义transform 所以参考如下链接写了一个这个\n",
    "# refer https://blog.csdn.net/ncc1995/article/details/91125964\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, transform):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "    \n",
    "def split_Train_Val_Data(data_dir, ratio, bs=4):\n",
    "    global train_len\n",
    "    global val_len\n",
    "    \"\"\" the sum of ratio must equal to 1\"\"\"\n",
    "    dataset = ImageFolder(data_dir)     # data_dir精确到分类目录的上一级\n",
    "    character = [[] for i in range(len(dataset.classes))]\n",
    "    print(dataset.class_to_idx)\n",
    "    for x, y in dataset.samples:  # 将数据按类标存放\n",
    "        character[y].append(x)\n",
    "#     print(dataset.samples)\n",
    "    train_inputs, val_inputs, test_inputs = [], [], []\n",
    "    train_labels, val_labels, test_labels = [], [], []\n",
    "    for i, data in enumerate(character):   # data为一类图片\n",
    "        num_sample_train = int(len(data) * ratio[0])\n",
    "        #print(num_sample_train)\n",
    "        num_sample_val = int(len(data) * ratio[1])\n",
    "        num_val_index = num_sample_train + num_sample_val\n",
    "        # 这里打乱一下数据，实验表明，不打乱也没事\n",
    "        random.seed(7)\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x in data[:num_sample_train]:\n",
    "            train_inputs.append(str(x))\n",
    "            train_labels.append(i)\n",
    "        for x in data[num_sample_train:num_val_index]:\n",
    "            val_inputs.append(str(x))\n",
    "            val_labels.append(i)\n",
    "    \n",
    "    train_len = len(train_inputs)\n",
    "    val_len = len(val_inputs)\n",
    "    print(\"train_length:%d,val length:%d\" %(train_len,val_len))\n",
    "    \n",
    "    train_dst = MyDataset(train_inputs, train_labels, train_transformer_ImageNet)\n",
    "    valid_dst = MyDataset(val_inputs, val_labels, val_transformer_ImageNet)\n",
    "    train_dataloader = DataLoader(train_dst,\n",
    "                                  batch_size=bs, shuffle=True)\n",
    "    val_dataloader = DataLoader(valid_dst,\n",
    "                                  batch_size=bs, shuffle=False)\n",
    " \n",
    "    return train_dataloader, val_dataloader\n",
    "# 定义pytorch的dataloader，数据划分0.9 \n",
    "data_loader = split_Train_Val_Data(data_dir,(0.9,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4500, 'val': 500}\n"
     ]
    }
   ],
   "source": [
    "# 为了保证后面和官方的baseline一致，所以可以这么写\n",
    "dataloders = {x:  data_loader[i] for i,x in enumerate(['train', 'val']) }\n",
    "dataset_sizes = {'train':train_len, 'val':val_len}\n",
    "print(dataset_sizes)\n",
    "# use gpu or not\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lossfunc, optimizer, scheduler, num_epochs=10):\n",
    "    start_time = time.time()\n",
    "    elapsed_time = 0\n",
    "    \n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "#         for phase in ['val']:\n",
    "\n",
    "            if phase == 'train':\n",
    "#                 \n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                \n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                \n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = lossfunc(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data\n",
    "                running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'val':\n",
    "                valid_acc.append(epoch_acc)\n",
    "            else:\n",
    "                train_acc.append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "        epch_model_name ='checkpiont/ep{}_train{}_val{}.pth'.format(epoch,train_acc[-1],valid_acc[-1],)\n",
    "\n",
    "        torch.save(model.state_dict(), epch_model_name)\n",
    "        print('model saved : ',epch_model_name)\n",
    "        elapsed_time = time.time() - start_time - elapsed_time\n",
    "        print('epoch complete in {:.0f}m {:.0f}s'.format(\n",
    "            elapsed_time // 60, elapsed_time % 60))\n",
    "\n",
    "        \n",
    "\n",
    "        # 这里使用了学习率调整策略\n",
    "        scheduler.step(valid_acc[-1])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        elapsed_time // 60, elapsed_time % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "  \n",
    "    return model,train_acc,valid_acc\n",
    "if os.path.exists('checkpiont')==False:\n",
    "    os.mkdir('checkpiont')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 之前手写的标签平滑可以直接用，也可以不用。\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# export label smoothing\n",
    "from torch.autograd import Variable\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n",
    "def lin_comb(a, b, epsilon):\n",
    "    return epsilon * a + b * (1 - epsilon)\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon, self.reduction = epsilon, reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lin_comb(loss / c, nll, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "采用resnet50神经网络结构训练模型,模型训练需要一定时间，等待该段代码运行完成后再往下执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/checkpiont\n",
      "rm: cannot remove '*': No such file or directory\n",
      "/home/ma-user/work\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Epoch 0/6\n",
      "----------\n",
      "train Loss: 0.4949 Acc: 0.4791\n",
      "val Loss: 0.4043 Acc: 0.7500\n",
      "model saved :  checkpiont/ep0_train0.47911110520362854_val0.7500000596046448.pth\n",
      "epoch complete in 7m 41s\n",
      "Epoch 1/6\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 冻结网络参数，训练最后一层\n",
    "\n",
    "# 删掉checkpiont文件\n",
    "%cd checkpiont \n",
    "!rm * \n",
    "%cd ..\n",
    "\n",
    "# 使用EfficientNet-b7模型\n",
    "model_ft = EfficientNet.from_name('efficientnet-b7') \n",
    "\n",
    "# 训练权重我自己下载的，然后拷贝到了notebook目录下面\n",
    "# 下载链接:https://github.com/lukemelas/EfficientNet-PyTorch/releases\n",
    "model_ft.load_state_dict(torch.load('pre.pth'))\n",
    "num_ftrs = model_ft._fc.in_features\n",
    "\n",
    "# 加入dropout，赋值过拟合\n",
    "model_ft._fc = nn.Sequential(nn.Dropout(0.4),nn.Linear(num_ftrs, 10))\n",
    "# \n",
    "# model_ft.load_state_dict(torch.load('checkpiont/ep2_train0.628000020980835_val0.7900000214576721.pth'))\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "# define loss function\n",
    "# lossfunc = nn.CrossEntropyLoss()\n",
    "lossfunc = LabelSmoothingCrossEntropy()\n",
    "\n",
    "# 冻结参数，只训练最后一层,训练10个epoch后解冻，训练全部参数\n",
    "parameters = list(model_ft._fc.parameters())\n",
    "# optimizer_ft = optim.SGD(parameters, lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# 使用Ranger优化器\n",
    "optimizer_ft = Ranger(parameters, 0.001,weight_decay=0)\n",
    "\n",
    "# 使用ReduceLROnPlateau学习调度器，如果2个epoch准确率没有提升，则减少学习率\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft,mode='max',patience=2,verbose=True)\n",
    "model_ft,train_acc,valid_acc = train_model(model=model_ft,\n",
    "                           lossfunc=lossfunc,\n",
    "                           optimizer=optimizer_ft,\n",
    "                           scheduler=exp_lr_scheduler,\n",
    "                           num_epochs=10)\n",
    "torch.save(model_ft.state_dict(), './fc_bestmodel.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.plot(train_acc,label=\"train\")\n",
    "plt.plot(valid_acc,label='valid')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/checkpiont\n",
      "/home/ma-user/work\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Epoch 0/19\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 解冻网络，训练全部参数\n",
    "\n",
    "\n",
    "# 删掉checkpiont文件\n",
    "%cd checkpiont \n",
    "!rm * \n",
    "%cd ..\n",
    "\n",
    "# 使用EfficientNet-b7模型\n",
    "model_ft = EfficientNet.from_name('efficientnet-b7') \n",
    "\n",
    "# 训练权重我自己下载的，然后拷贝到了notebook目录下面\n",
    "# 下载链接:https://github.com/lukemelas/EfficientNet-PyTorch/releases\n",
    "# model_ft.load_state_dict(torch.load('eff.pth'))\n",
    "num_ftrs = model_ft._fc.in_features\n",
    "\n",
    "# 加入dropout，赋值过拟合\n",
    "model_ft._fc = nn.Sequential(nn.Dropout(0.4),nn.Linear(num_ftrs, 10))\n",
    "\n",
    "# model_ft.load_state_dict(torch.load('fc_bestmodel.pth'))\n",
    "model_ft.load_state_dict(torch.load('fc_bestmodel.pth'))\n",
    "\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "# define loss function\n",
    "# lossfunc = nn.CrossEntropyLoss()\n",
    "lossfunc = LabelSmoothingCrossEntropy()\n",
    "\n",
    "# 训练全部参数\n",
    "parameters = list(model_ft.parameters())\n",
    "# optimizer_ft = optim.SGD(parameters, lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# 使用Ranger优化器\n",
    "optimizer_ft = Ranger(parameters, 0.001,weight_decay=0.0005)\n",
    "\n",
    "# 使用ReduceLROnPlateau学习调度器，如果2个epoch准确率没有提升，则减少学习率\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft,mode='max',patience=2,verbose=True)\n",
    "model_ft,train_acc,valid_acc = train_model(model=model_ft,\n",
    "                           lossfunc=lossfunc,\n",
    "                           optimizer=optimizer_ft,\n",
    "                           scheduler=exp_lr_scheduler,\n",
    "                           num_epochs=10)\n",
    "torch.save(model_ft.state_dict(), './all_bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.plot(train_acc,label=\"train\")\n",
    "plt.plot(valid_acc,label='valid')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练好的模型保存下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的模型保存至OBS\n",
    "将模型保存到OBS桶中model文件夹下，为后续推理测试、模型提交做准备。将如下代码中\"obs-aifood-baseline\"修改成您OBS桶的名称。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls checkpiont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意拷贝到自己的目录环境下面即可\n",
    "mox.file.copy('fc_bestmodel.pth',\n",
    "              's3://xxx/0514/out/model/fc_97.6post.pth')\n",
    "\n",
    "mox.file.copy('all_bestmodel.pth',\n",
    "              's3://xxx/0514/out/model/all_model.pth')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
